import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

import os
from dotenv import load_dotenv
import tempfile
import boto3

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="ã‚¢ãƒ‹ãƒ¡ãƒ‰ãƒ©ãˆã‚‚ã‚“ ã‚ã‚‰ã™ã˜æ¤œç´¢", page_icon="ğŸ“š")

st.title("ã‚¢ãƒ‹ãƒ¡ãƒ‰ãƒ©ãˆã‚‚ã‚“ ã‚ã‚‰ã™ã˜æ¤œç´¢")
st.markdown("ã‚¢ãƒ‹ãƒ¡ãƒ‰ãƒ©ãˆã‚‚ã‚“ã§æ”¾é€ã•ã‚ŒãŸãŠã¯ãªã—ã‚’, ç™»å ´äººç‰©ã‚„è¦šãˆã¦ã„ã‚‹è©±ã®å†…å®¹ã‹ã‚‰æ¤œç´¢ã§ãã¾ã™")
st.markdown("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: [ãƒ‰ãƒ©ãˆã‚‚ã‚“ ãŠã¯ãªã—ãƒªã‚¹ãƒˆ](https://www.tv-asahi.co.jp/doraemon/story/bk/)")

# --- FAISS èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_vectorstore():
    load_dotenv()
    OPENAPI_API_KEY = os.getenv("chatgpt_secret")
    S3_AWS_REGION = os.getenv("S3_AWS_REGION")
    S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME_ANIME")
    IAM_ACCESS_KEY = os.getenv("IAM_ACCESS_KEY")
    IAM_SECRET_ACCESS_KEY = os.getenv("IAM_SECRET_ACCESS_KEY")

    embedding_model = OpenAIEmbeddings(openai_api_key=OPENAPI_API_KEY)

    # S3ã‹ã‚‰FAISSãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    s3 = boto3.client(
        "s3",
        region_name=S3_AWS_REGION,
        aws_access_key_id=IAM_ACCESS_KEY,
        aws_secret_access_key=IAM_SECRET_ACCESS_KEY
    )

    with tempfile.TemporaryDirectory() as tmpdir:
        files_to_download = ["index.faiss", "index.pkl"]  # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«
        for file in files_to_download:
            s3.download_file(S3_BUCKET_NAME, file, os.path.join(tmpdir, file))

        # FAISSã‚’èª­ã¿è¾¼ã¿
        vectorstore = FAISS.load_local(
            tmpdir,
            embeddings=embedding_model,
            allow_dangerous_deserialization=True,
        )
        return vectorstore


vectorstore = load_vectorstore()

# --- ã‚¯ã‚¨ãƒªå…¥åŠ›æ¬„ ---
query = st.text_area("ğŸ” ãŠã¯ãªã—ã®ã‚ã‚‰ã™ã˜ã‚„ç‰¹å¾´ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=100)

# --- æ¤œç´¢ãƒœã‚¿ãƒ³ ---
if st.button("æ¤œç´¢"):
    if not query.strip():
        st.warning("å…¥åŠ›ãŒç©ºã§ã™ã€‚ã‚ã‚‰ã™ã˜ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("æ¤œç´¢ä¸­..."):
            results = vectorstore.similarity_search_with_score(query, k=3)

        st.subheader("ğŸ” é¡ä¼¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        for i, (doc, score) in enumerate(results, 1):
            meta = doc.metadata
            title = meta.get("title", "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
            broadcasting_date = meta.get("broadcasting_date", "ä¸æ˜")
            story_index = meta.get("index", "ä¸æ˜")
            summary = doc.page_content.strip()

            with st.expander(f"{i}. {title}ï¼ˆè©±æ•°: {story_index}, é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {score:.4f}ï¼‰"):
                st.markdown("**æ”¾é€æ—¥**: " + broadcasting_date)
                st.markdown("**è¦ç´„**:")
                st.write(summary)
